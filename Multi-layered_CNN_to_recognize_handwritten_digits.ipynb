{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layered CNN to recognize handwritten digits\n",
    "## Based on the O'Reilly Tutorial: «Building deep learning neural networks using TensorFlow layers»\n",
    "### A step-by-step tutorial on how to use TensorFlow to build a multi-layered convolutional network\n",
    "### by Barbara / Basia Fusinska February 14, 2018\n",
    "\n",
    "#### https://www.oreilly.com/ideas/building-deep-learning-neural-networks-using-tensorflow-layers\n",
    "\n",
    "### Major rework, code completion, restructuration and iPython Notebook by Claude COULOMBE - PhD candidate - TÉLUQ / UQAM - Montréal\n",
    "\n",
    "With a lot of work I was able to build something working like a step-by-step tutorial from the original code hodgepodge which in part is coming from other TensorFlow tutorials. \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deep learning has proven its effectiveness in many fields, such as computer vision, natural language processing (NLP), text translation, or speech to text. It takes its name from the high number of layers used to build the neural network performing machine learning tasks. There are several types of layers as well as overall network architectures, but the general rule holds that the deeper the network is, the more complexity it can grasp. This article will explain fundamental concepts of neural network layers and walk through the process of creating several types using <a href=\"https://www.tensorflow.org/\">TensorFlow</a>.\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "TensorFlow is the platform that contributed to making artificial intelligence (AI) available to the broader public. It’s an open source library with a vast community and great support. TensorFlow provides a set of tools for building neural network architectures, and then training and serving the models. It offers different levels of abstraction, so you can use it for cut-and-dried machine learning processes at a high level or go more in-depth and write the low-level calculations yourself.\n",
    "\n",
    "TensorFlow offers many kinds of layers in its <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers\"> tf.layers</a> package. The module makes it easy to create a layer in the deep learning model without going into many details. At the moment, it supports types of layers used mostly in <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">convolutional networks</a>. For other types of networks, like RNNs, you may need to look at <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/rnn\">tf.contrib.rnn</a> or <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn\">tf.nn</a>. The most basic type of layer is the <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\">fully connected</a> one. To implement it, you only need to set up the input and the size in the <a href=\"https://www.tensorflow.org/api_docs/python/tf/layers/Dense\">Dense class</a>. Other kinds of layers might require more parameters, but they are implemented in a way to cover the default behaviour and spare the developers’ time.\n",
    "\n",
    "There is some disagreement on what a layer is and what it is not. One opinion states that a layer must store trained parameters (like weights and biases). This means, for instance, that applying the activation function is not another layer. Indeed, `tf.layers` implements such a function by using the activation parameter. Layers introduced in the module don’t always strictly follow this rule, though. You can find a large range of types there: fully connected, convolution, pooling, flatten, batch normalization, dropout, and convolution transpose. It may seem that, for example, layer flattening and max pooling don’t store any parameters trained in the learning process. Nonetheless, they are performing more complex operations than activation function, so the authors of the module decided to set them up as separate classes. Later in the article, we’ll discuss how to use some of them to build a deep convolutional network.\n",
    "\n",
    "### Convolutional neural network\n",
    "\n",
    "A typical convolutional network is a sequence of convolution and pooling pairs, followed by a few fully connected layers. A convolution is like a small neural network that is applied repeatedly, once at each location on its input. As a result, the network layers become much smaller but increase in depth. < a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling\">Pooling</a> is the operation that usually decreases the size of the input image. Max pooling is the most common pooling algorithm, and has proven to be effective in many computer vision tasks.\n",
    "\n",
    "In this article, I’ll show the use of TensorFlow in applying a convolutional network to image processing, using the <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST data set</a> for our example. The task is to recognize a digit ranging from 0 to 9 from its handwritten representation.\n",
    "\n",
    "## Code\n",
    "\n",
    "#### Load the data\n",
    "\n",
    "Download the MNIST data in .png format: https://github.com/myleott/mnist_png/blob/master/mnist_png.tar.gz\n",
    "\n",
    "\n",
    "First, TensorFlow has the capabilities to load the data. All you need to do is to use the input_data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/claudecoulombe/git/NanodegreeFoundationDeepLearning/my-deep-learning/face_generation/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/claudecoulombe/git/NanodegreeFoundationDeepLearning/my-deep-learning/face_generation/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/claudecoulombe/git/NanodegreeFoundationDeepLearning/my-deep-learning/face_generation/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/claudecoulombe/git/NanodegreeFoundationDeepLearning/my-deep-learning/face_generation/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "folder_path = \"/Users/claudecoulombe/git/NanodegreeFoundationDeepLearning/my-deep-learning/face_generation/data/mnist/\"\n",
    "\n",
    "mnist = input_data.read_data_sets(folder_path, one_hot=True)\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "test_images = mnist.test.images # Returns np.array\n",
    "test_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to build a multilayered architecture. After describing the learning process, I’ll walk you through the creation of different kinds of layers and apply them to the MNIST classification task.\n",
    "\n",
    "### Define input / output\n",
    "For the actual training, let's start simple and create the network with just one output layer. We begin by defining placeholders for the input data and labels. During the training phase, they will be filled with the data from the MNIST data set. Because the data was flattened, the input layer has only one dimension. The size of the output layer corresponds to the number of labels. Both input and labels have the additional dimension set to None, which will handle the variable number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "print(train_labels.shape)\n",
    "labels_size = train_labels.shape[1]\n",
    "# Warning: change input to inputs because the Python built-in function input\n",
    "inputs = tf.placeholder(tf.float32, [None, image_size*image_size])\n",
    "labels = tf.placeholder(tf.float32, [None, labels_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to build the exciting part: the output layer. The magic behind it is quite straightforward. Every neuron in it has the weight and bias parameters, gets the data from every input, and performs some calculations. This is what makes it a fully connected layer.\n",
    "\n",
    "TensorFlow’s `tf.layers` package allows you to formulate all this in just one line of code. All you need to provide is the input and the size of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = tf.layers.dense(inputs=inputs, units=labels_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimizer\n",
    "\n",
    "The training process works by optimizing the loss function, which measures the difference between the network predictions and actual labels' values. Deep learning often uses a technique called <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\">cross entropy</a> to define the loss.\n",
    "\n",
    "TensorFlow provides the function called <a href=\"https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy\">tf.losses.softmax_cross_entropy</a> that internally applies the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\">softmax algorithm</a> on the model's unnormalized prediction and sums results across all classes. In our example, we use the <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam\">Adam optimizer</a> provided by the `tf.train` API. `labels` will be provided in the process of training and testing, and will represent the underlying truth. `output` represents the network predictions and will be defined in the next section when building the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(labels, output)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define performance evaluation\n",
    "To evaluate the performance of the training process, we want to compare the output with the real labels and calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now, we’ll introduce a simple training process using batches and a fixed number of steps and learning rate. For the MNIST data set, the `next_batch` function would just call `mnist.train.next_batch`. After the network is trained, we can check its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training batch accuracy 0.08\n",
      "Step 100, training batch accuracy 0.26\n",
      "Step 200, training batch accuracy 0.57\n",
      "Step 300, training batch accuracy 0.69\n",
      "Step 400, training batch accuracy 0.77\n",
      "Step 500, training batch accuracy 0.76\n",
      "Test accuracy: 0.7846\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "# Open the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# You should define batch size steps\n",
    "steps = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "for i in range(steps):\n",
    "# Get the next batch\n",
    "    # Take the built-in next_batch function\n",
    "    input_batch, labels_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "    feed_dict = {inputs: input_batch, labels: labels_batch}\n",
    "    \n",
    "    # Print the current batch accuracy every 100 steps\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        print(\"Step %d, training batch accuracy %g\"%(i, train_accuracy))\n",
    "    # Run the optimization step\n",
    "    train_step.run(feed_dict=feed_dict)\n",
    "\n",
    "# Print the test accuracy once the training is over\n",
    "print(\"Test accuracy: %g\"%accuracy.eval(feed_dict={inputs: test_images, labels: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysis of the results\n",
    "\n",
    "Our first network isn't that impressive in regard to accuracy, less than 80%. But it’s simple, so it runs very fast.\n",
    "\n",
    "We’ll try to improve our network by adding more layers between the input and output. These are called hidden layers. First, we add another fully connected one.\n",
    "\n",
    "### Adding one hidden layer\n",
    "Some minor changes are needed from the previous architecture. First of all, there is another parameter indicating the number of neurons of the hidden layer. The definition itself takes the input data and connects to the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(inputs=inputs, units=1024, activation=tf.nn.relu)\n",
    "output = tf.layers.dense(inputs=hidden, units=labels_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training batch accuracy 0.1\n",
      "Step 100, training batch accuracy 0.11\n",
      "Step 200, training batch accuracy 0.09\n",
      "Step 300, training batch accuracy 0.09\n",
      "Step 400, training batch accuracy 0.07\n",
      "Step 500, training batch accuracy 0.08\n",
      "Test accuracy: 0.0869\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Open the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# You should define batch size steps\n",
    "steps = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "for i in range(steps):\n",
    "# Get the next batch\n",
    "    # Take the built-in next_batch function\n",
    "    input_batch, labels_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "    feed_dict = {inputs: input_batch, labels: labels_batch}\n",
    "    \n",
    "    # Print the current batch accuracy every 100 steps\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        print(\"Step %d, training batch accuracy %g\"%(i, train_accuracy))\n",
    "    # Run the optimization step\n",
    "    train_step.run(feed_dict=feed_dict)\n",
    "\n",
    "# Print the test accuracy once the training is over\n",
    "print(\"Test accuracy: %g\"%accuracy.eval(feed_dict={inputs: test_images, labels: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time, we used an activation parameter. It runs whatever comes out of the neuron through the activation function, which in this case is <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\">ReLU</a>. This algorithm has been proven to work quite well with deep architectures.\n",
    "\n",
    "You should see a slight decrease in performance. Our network is becoming deeper, which means it's getting more parameters to be tuned, and this makes the training process longer. \n",
    "\n",
    "<u>The accuracy is very poor for one epoch (only one complete training step). In fact the accuracy is less than 10 %</u>\n",
    "\n",
    "### Training over many epochs\n",
    "On the other hand, if you're performing many epochs, this will improve the accuracy significantly, over the 90% level. \n",
    "With just 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.409\n",
      "Accuracy 0.900818\n",
      "Epoch: 2 cost = 0.188\n",
      "Accuracy 0.9488\n",
      "Epoch: 3 cost = 0.137\n",
      "Accuracy 0.962727\n",
      "Epoch: 4 cost = 0.106\n",
      "Accuracy 0.971836\n",
      "Epoch: 5 cost = 0.085\n",
      "Accuracy 0.977382\n",
      "Test accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(labels, output)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# Open the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    sum_cost = 0\n",
    "    sum_accuracy = 0\n",
    "    # You should define batch size steps\n",
    "    steps = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "    for i in range(steps):\n",
    "    # Get the next batch\n",
    "        # Take the built-in next_batch function\n",
    "        input_batch, labels_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "        feed_dict = {inputs: input_batch, labels: labels_batch}\n",
    "        _, cost = sess.run([train_step,loss],feed_dict=feed_dict)\n",
    "        sum_cost += cost\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        sum_accuracy += train_accuracy\n",
    "        # Run the optimization step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "    print(\"Epoch:\", (epoch), \"cost =\", \"{:.3f}\".format(sum_cost/steps))\n",
    "    print(\"Accuracy %g\"%(sum_accuracy/steps))\n",
    "\n",
    "# Print the test accuracy once the training is over\n",
    "print(\"Test accuracy: %g\"%accuracy.eval(feed_dict={inputs: test_images, labels: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a first convolutional layers\n",
    "The next two layers we're going to add are the integral parts of convolutional networks. They work differently from the dense ones and perform especially well with input that has two or more dimensions (such as images). The parameters of the convolutional layer are the size of the convolution window and the number of filters. A padding set of `same` indicates that the resulting layer is of the same size. After this step, we apply max pooling.\n",
    "\n",
    "Using convolution allows us to take advantage of the 2D representation of the input data. We'd lost it when we flattened the digits pictures and fed the resulting data into the dense layer. To go back to the original structure, we can use the `tf.reshape` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input2d = tf.reshape(inputs, [-1,image_size,image_size,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for convolution and max pooling follows. Notice that for the next connection with the dense layer, the output must be flattened back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(inputs=input2d, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "pool_flat = tf.reshape(pool1, [-1, 14 * 14 * 32])\n",
    "hidden = tf.layers.dense(inputs= pool_flat, units=1024, activation=tf.nn.relu)\n",
    "output = tf.layers.dense(inputs=hidden, units=labels_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost = 0.031\n",
      "Accuracy 0.9918\n",
      "Epoch: 10 cost = 0.009\n",
      "Accuracy 0.998055\n",
      "Epoch: 15 cost = 0.003\n",
      "Accuracy 0.999655\n",
      "Epoch: 20 cost = 0.004\n",
      "Accuracy 0.999673\n",
      "Test accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(labels, output)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# Open the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    sum_cost = 0\n",
    "    sum_accuracy = 0\n",
    "    # You should define batch size steps\n",
    "    steps = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "    for i in range(steps):\n",
    "    # Get the next batch\n",
    "        # Take the built-in next_batch function\n",
    "        input_batch, labels_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "        feed_dict = {inputs: input_batch, labels: labels_batch}\n",
    "        _, cost = sess.run([train_step,loss],feed_dict=feed_dict)\n",
    "        sum_cost += cost\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        sum_accuracy += train_accuracy\n",
    "        # Run the optimization step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "    if epoch%5 == 0:\n",
    "        print(\"Epoch:\", (epoch), \"cost =\", \"{:.3f}\".format(sum_cost/steps))\n",
    "        print(\"Accuracy %g\"%(sum_accuracy/steps))\n",
    "\n",
    "# Print the test accuracy once the training is over\n",
    "print(\"Test accuracy: %g\"%accuracy.eval(feed_dict={inputs: test_images, labels: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layers\n",
    "\n",
    "Adding the convolution to the picture increases the accuracy even more (to 97%), but slows down the training process significantly. To take full advantage of the model, we should continue with another layer. We again are using the 2D input, but flattening only the output of the second layer. The first one doesn't need flattening now because the convolution works with higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "pool_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you need be quite patient when running the code. The complexity of the network is adding a lot of overhead, but we are rewarded with better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 cost = 0.055\n",
      "Accuracy 0.0262364\n",
      "Epoch: 5 cost = 0.011\n",
      "Accuracy 0.0334727\n",
      "Epoch: 10 cost = 0.008\n",
      "Accuracy 0.0342545\n",
      "Epoch: 15 cost = 0.006\n",
      "Accuracy 0.0348\n",
      "Epoch: 20 cost = 0.005\n",
      "Accuracy 0.0349273\n",
      "Test accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(labels, output)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# Open the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    sum_cost = 0\n",
    "    sum_accuracy = 0\n",
    "    # You should define batch size steps\n",
    "    steps = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "    for i in range(1,epochs+1):\n",
    "    # Get the next batch\n",
    "        # Take the built-in next_batch function\n",
    "        input_batch, labels_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "        feed_dict = {inputs: input_batch, labels: labels_batch}\n",
    "        _, cost = sess.run([train_step,loss],feed_dict=feed_dict)\n",
    "        sum_cost += cost\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        sum_accuracy += train_accuracy\n",
    "        # Run the optimization step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "    if epoch%5 == 0:\n",
    "        print(\"Epoch:\", (epoch), \"cost =\", \"{:.3f}\".format(sum_cost/steps))\n",
    "        print(\"Accuracy %g\"%(sum_accuracy/steps))\n",
    "\n",
    "# Print the test accuracy once the training is over\n",
    "print(\"Test accuracy: %g\"%accuracy.eval(feed_dict={inputs: test_images, labels: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization using Dropout\n",
    "\n",
    "We'll now introduce another technique that could improve the network performance and avoid overfitting. It's called <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout\">Dropout</a>, and we’ll apply it to the hidden dense layer. Dropout works in a way that individual nodes are either shut down or kept with some explicit probability. It is used in the training phase, so remember you need to turn it off when evaluating your network.\n",
    "\n",
    "To use Dropout, we need to change the code slightly. First of all, we need a placeholder to be used in both the training and testing phases to hold the probability of the Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "should_drop = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to define the dropout and connect it to the output layer. The rest of the architecture stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(inputs=pool_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs=hidden, rate=0.5, training=should_drop)\n",
    "output = tf.layers.dense(inputs=dropout, units=labels_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost = 0.036\n",
      "Accuracy 0.989927\n",
      "Epoch: 10 cost = 0.012\n",
      "Accuracy 0.996545\n",
      "Epoch: 15 cost = 0.006\n",
      "Accuracy 0.998382\n",
      "Epoch: 20 cost = 0.004\n",
      "Accuracy 0.999055\n",
      "Test accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(labels, output)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# Open the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    sum_cost = 0\n",
    "    sum_accuracy = 0\n",
    "    # You should define batch size steps\n",
    "    steps = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "    for i in range(steps):\n",
    "    # Get the next batch\n",
    "        # Take the built-in next_batch function\n",
    "        input_batch, labels_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "        feed_dict = {inputs: input_batch, labels: labels_batch, should_drop:True}\n",
    "        _, cost = sess.run([train_step,loss],feed_dict=feed_dict)\n",
    "        sum_cost += cost\n",
    "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        sum_accuracy += train_accuracy\n",
    "        # Run the optimization step\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "    if epoch%5 == 0:\n",
    "        print(\"Epoch:\", (epoch), \"cost =\", \"{:.3f}\".format(sum_cost/steps))\n",
    "        print(\"Accuracy %g\"%(sum_accuracy/steps))\n",
    "\n",
    "# Print the test accuracy once the training is over\n",
    "print(\"Test accuracy: %g\"%accuracy.eval(feed_dict={inputs: test_images, labels: test_labels, should_drop:False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this article, we started by introducing the concepts of deep learning and used TensorFlow to build a multi-layered convolutional network. The code can be reused for image recognition tasks and applied to any data set. More complex images, however, would require greater depth as well as more sophisticated twists, such as inception or ResNets.\n",
    "\n",
    "The key lesson from this exercise is that you don’t need to master statistical techniques or write complex matrix multiplication code to create an AI model. TensorFlow can handle those for you. However, you need to know which algorithms are appropriate for your data and application, and determine the best hyperparameters, such as network architecture, depth of layers, batch size, learning rate, etc. Be aware that the variety of choices in libraries like TensorFlow give you requires a lot of responsibility on your side.\n",
    "\n",
    "Note from C.Coulombe: Don't be fooled by the 99% of accuracy on tests data, MNIST is one of the cleanest dataset that you could get for handwritten digit recognition. You should expect much less accuracy on less clean data.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
